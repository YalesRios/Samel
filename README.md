# Samel Plan

## Datasets/Problems

- Endangered Animals

## Ensemble types

- Dropout Ensembles

- Bagging

- Boosting

- Snapshot Ensembles

- AdaNet

## Fusion types

- Majority Voting

- Unweighted Average

- Super Learner

- Stacked Genralisation (should be more specific)

- Bayes Optimal Classifier

## Architectures

- NIN

- GoogLeNet

- VGG net

- Res Net

## Saliency Methods

- Gradient Input

- Competitive Gradient Input

- Integrated Gradients

- Layerwise Relevance Propagation

- Taylor Decomposition

- DeepLIFT

> Kindermans et al. [6] and Shrikumar et al. [3] showed that if modifications for numerical stability are not taken into account, the LRP rules are equivalent within a scaling factor to Gradient ⊙ Input. Ancona et al. [7] showed that for ReLU networks (with zero baseline and no biases) the ε-LRP and DeepLIFT (Rescale) explanation methods are equivalent to the Gradient ⊙ Input.

[Gupta, Arushi, and Sanjeev Arora. "A Simple Saliency Method That Passes the Sanity Checks." arXiv preprint arXiv:1905.12152 (2019).](https://arxiv.org/abs/1905.12152)

